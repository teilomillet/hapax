server:
  port: 8080
  read_timeout: 30s
  write_timeout: 30s
  max_header_bytes: 1048576
  shutdown_timeout: 5s

llm:
  provider: anthropic
  model: claude-3.5-haiku-latest
  api_key: ${ANTHROPIC_API_KEY}
  max_context_tokens: 100000
  retry:
    max_retries: 3
    initial_delay: 100ms
    max_delay: 2s
    multiplier: 2.0
    retryable_errors: ["rate_limit", "timeout", "server_error"]

providers:
  anthropic:
    type: anthropic
    model: claude-3.5-haiku-latest
    api_key: ${ANTHROPIC_API_KEY}
  ollama:
    type: ollama
    model: llama3
    api_key: ""

provider_preference:
  - anthropic
  - ollama

logging:
  level: info
  format: json

metrics:
  enabled: true
  prometheus:
    enabled: true

routes:
  - path: /v1/completions
    handler: completion
    version: v1
    methods: [POST]
  - path: /health
    handler: health
    version: v1
    methods: [GET]

processing:
  request_templates:
    default: "{{.Input}}"
    chat: "{{range .Messages}}{{.Role}}: {{.Content}}\n{{end}}"
  response_formatting:
    clean_json: true
    trim_whitespace: true
    max_length: 1048576